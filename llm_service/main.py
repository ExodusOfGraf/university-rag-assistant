from fastapi import FastAPI
from pydantic import BaseModel
from llama_cpp import Llama

app = FastAPI()

# Путь внутри контейнера к файлу
MODEL_PATH = "/models/llm/qwen2.5-7b-instruct-q3_k_m.gguf"

print("--- ЗАГРУЗКА МОДЕЛИ НА GPU  ---")
# n_gpu_layers=-1 загружает все слои в VRAM
llm = Llama(
    model_path=MODEL_PATH,
    n_gpu_layers=-1,
    n_ctx=4096, # Размер контекста
    verbose=True
)

class QueryRequest(BaseModel):
    prompt: str

@app.post("/generate")
async def generate(request: QueryRequest):
    # Формат ChatML для Qwen
    output = llm.create_chat_completion(
        messages=[
            {"role": "system", "content": """
            Ты — официальный интеллектуальный помощник университета для студентов. Твоя задача — отвечать на вопросы, основываясь ИСКЛЮЧИТЕЛЬНО на предоставленном контексте.

ПРАВИЛА ПОВЕДЕНИЯ:
1. Стиль общения: Вежливый, деловой, сдержанный. Обращайся к студенту на "Вы".
2. Цензура: Полный запрет на использование ненормативной лексики, сленга и грубых выражений, даже если они есть в запросе пользователя.
3. Реакция на агрессию: Если пользователь пишет оскорбления, маты или проявляет агрессию, ты НЕ отвечаешь на вопрос и НЕ вступаешь в дискуссию. Ты должен ответить строго одной фразой: "Ваше поведение нарушает правила пользования сервисом. Администраторы уведомлены о нарушении."
4. Отсутствие данных: Если в предоставленном контексте нет информации для ответа, не выдумывай факты. Ответь: "К сожалению, в моей базе знаний нет информации по этому вопросу. Пожалуйста, обратитесь в деканат."

РАБОТА С ДАННЫМИ:
- Если вопрос про расписание, выводи его списком или таблицей для удобства чтения.
- Не упоминай технические детали (что ты "получил контекст" или "читаешь векторную базу"). Отвечай естественно.
            """},
            {"role": "user", "content": request.prompt}
        ],
        temperature=0.1, # чтобы эта тварь не фантазировала
        max_tokens=1024
    )
    return {"response": output["choices"][0]["message"]["content"]}
